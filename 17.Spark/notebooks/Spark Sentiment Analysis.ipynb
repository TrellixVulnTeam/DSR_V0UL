{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clones Spark NLP repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/JohnSnowLabs/spark-nlp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.base import DocumentAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As of version 1.5.3, the Finisher transformer has a bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sparknlp.base import AnnotatorTransformer\n",
    "\n",
    "class Finisher(AnnotatorTransformer):\n",
    "\n",
    "    inputCols = Param(Params._dummy(), \"inputCols\", \"input annotations\", typeConverter=TypeConverters.toListString)\n",
    "    outputCols = Param(Params._dummy(), \"outputCols\", \"output finished annotation cols\", typeConverter=TypeConverters.toListString)\n",
    "    valueSplitSymbol = Param(Params._dummy(), \"valueSplitSymbol\", \"character separating annotations\", typeConverter=TypeConverters.toString)\n",
    "    annotationSplitSymbol = Param(Params._dummy(), \"annotationSplitSymbol\", \"character separating annotations\", typeConverter=TypeConverters.toString)\n",
    "    cleanAnnotations = Param(Params._dummy(), \"cleanAnnotations\", \"whether to remove annotation columns\", typeConverter=TypeConverters.toBoolean)\n",
    "    #includeMetadata = Param(Params._dummy(), \"includeMetadata\", \"annotation metadata format\", typeConverter=TypeConverters.toBoolean)\n",
    "    outputAsArray = Param(Params._dummy(), \"outputAsArray\", \"finisher generates an Array with the results instead of string\", typeConverter=TypeConverters.toBoolean)\n",
    "    name = \"Finisher\"\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(Finisher, self).__init__(classname=\"com.johnsnowlabs.nlp.Finisher\")\n",
    "        self._setDefault(\n",
    "            valueSplitSymbol=\"#\",\n",
    "            annotationSplitSymbol=\"@\",\n",
    "            cleanAnnotations=True,\n",
    "            #includeMetadata=False,\n",
    "            outputAsArray=False\n",
    "        )\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setInputCols(self, value):\n",
    "        return self._set(inputCols=value)\n",
    "\n",
    "    def setOutputCols(self, value):\n",
    "        return self._set(outputCols=value)\n",
    "\n",
    "    def setValueSplitSymbol(self, value):\n",
    "        return self._set(valueSplitSymbol=value)\n",
    "\n",
    "    def setAnnotationSplitSymbol(self, value):\n",
    "        return self._set(annotationSplitSymbol=value)\n",
    "\n",
    "    def setCleanAnnotations(self, value):\n",
    "        return self._set(cleanAnnotations=value)\n",
    "\n",
    "    def setIncludeKeys(self, value):\n",
    "        return self._set(includeMetadata=value)\n",
    "\n",
    "    def setOutputAsArray(self, value):\n",
    "        return self._set(outputAsArray=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark. \\\n",
    "        read. \\\n",
    "        parquet(\"./spark-nlp/src/test/resources/sentiment.parquet\"). \\\n",
    "        limit(10000)\n",
    "data.cache()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create appropriate annotators. We are using Sentence Detection, Tokenizing the sentences, and find the lemmas of those tokens\n",
    "The Finisher will only output the Sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "lemmatizer = Lemmatizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\") \\\n",
    "    .setDictionary(\"./spark-nlp/src/test/resources/lemma-corpus-small/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "        \n",
    "sentiment_detector = SentimentDetector() \\\n",
    "    .setInputCols([\"lemma\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment_score\") \\\n",
    "    .setDictionary(\"./spark-nlp/src/test/resources/sentiment-corpus/default-sentiment-dict.txt\", \",\")\n",
    "    \n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment_score\"]) \\\n",
    "    .setOutputCols([\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the pipeline, which is only being trained from external resources, not from the dataset we pass on.\n",
    "The prediction runs on the target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, lemmatizer, sentiment_detector, finisher])\n",
    "model = pipeline.fit(data)\n",
    "result = model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.filter(\"sentiment != 'positive'\").limit(50).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the finisher output, to find the positive sentiment lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import functions as F\n",
    "#result.withColumn('sentiment', F.expr('sentiment_score[0].result')).filter(\"sentiment == 'positive'\").select('text').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-08 16:59:18,656] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7efeeb4f0198>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1.0):\n",
    "    # initialize value tables with zeros\n",
    "    value_table=np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # setting number of iterations and thresold \n",
    "    num_iterations=100000\n",
    "    thresold=1e-20\n",
    "    for i in range(num_iterations):\n",
    "    \n",
    "        # copying the value table to the updated value_table\n",
    "        updated_value_table=np.copy(value_table)\n",
    "    \n",
    "        for state in range(env.observation_space.n):\n",
    "            Q_value=[]\n",
    "            for action in range(env.action_space.n):\n",
    "                next_states_rewards=[] # next state rewards store the Q_Value for each action\n",
    "                for next_s in env.P[state][action]:\n",
    "                    # env.P gets the model of the env: (p(s'|s,a),s',r(s'|s,a))\n",
    "                    trans_prob, next_state, reward_prob, _=next_s\n",
    "                    next_states_rewards.append(trans_prob*(reward_prob+gamma*updated_value_table[next_state]))\n",
    "                Q_value.append(np.sum(next_states_rewards))\n",
    "                # Q_value <-sum over s' (p(s'|s,a)*(r(s'|s,a)+gamma*V(s')))\n",
    "            \n",
    "            value_table[state]=max(Q_value)\n",
    "            # Q_value is a list of [q(state, a') for all a' in action space]\n",
    "            # V_updated (state) <- max over actions (Q(state, action))\n",
    "        \n",
    "        if(np.sum(np.fabs(updated_value_table-value_table))<=thresold):\n",
    "            print('Value Iteration converging at iteration:{}'.format(i+1))\n",
    "            break\n",
    "            \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_policy(env, value_table, gamma=1.0):\n",
    "    # initializing the policy table\n",
    "    policy=np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        \n",
    "        # initializing the Q_table for each state\n",
    "        Q_table=np.zeros(env.action_space.n)\n",
    "        \n",
    "        # compute Q Value for each actions of the state\n",
    "        #Q_value(state,action) <-sum over s' (p(s'|s,a)*(r(s'|s,a)+gamma*V(s')))\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_s in env.P[state][action]:\n",
    "                trans_prob, next_state, reward_prob, _=next_s\n",
    "                # adding the next_state_rewards for the succesor state next_state to the Q_value\n",
    "                Q_table[action]+=(trans_prob*(reward_prob+gamma*value_table[next_state]))\n",
    "             \n",
    "        #select the action that has the maximum Q-value for finding the optimal policy\n",
    "        policy[state]=np.argmax(Q_table)\n",
    "        #pi(state)<-argmax over actions (Q_table(state, action))\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converging at iteration:267\n"
     ]
    }
   ],
   "source": [
    "optimal_value_function=value_iteration(env=env, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0688909   0.06141457  0.07440976  0.05580732  0.09185454  0.\n",
      "  0.11220821  0.          0.14543635  0.24749695  0.29961759  0.          0.\n",
      "  0.3799359   0.63902015  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_policy=extract_policy(optimal_value_function, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  3.  0.  3.  0.  0.  0.  0.  3.  1.  0.  0.  0.  2.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-08 16:59:18,882] Making new env: FrozenLakeNotSlippery-v0\n"
     ]
    }
   ],
   "source": [
    "env_d=gym.make('FrozenLakeNotSlippery-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converging at iteration:7\n"
     ]
    }
   ],
   "source": [
    "optimal_value_function_d=value_iteration(env=env_d, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_policy_d=extract_policy(env_d, optimal_value_function_d,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  1.  0.  1.  0.  1.  0.  2.  1.  1.  0.  0.  2.  2.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 0, 0.0, False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_d.P[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Recommender\n",
    "\n",
    "## Intro\n",
    "\n",
    "The purpose of this exercise is to use Spark in a real dataset, instead of just a toy example.\n",
    "\n",
    "You will use the data from the [Yelp Dataset Challenge](https://www.yelp.de/dataset_challenge), which contains information about businesses, users, reviews and more.\n",
    "\n",
    "For this exercise, you will need to focus only on the following files:\n",
    "- yelp_academic_dataset_business.json\n",
    "- yelp_academic_dataset_review.json\n",
    "\n",
    "The goal is to build a recommender using [Spark's ALS (Alternating Least Squares)](https://spark.apache.org/docs/2.3.0/ml-collaborative-filtering.html) and then generate recommendations for a given user.\n",
    "\n",
    "Since the dataset is quite big, you should pick a business category (e.g. Restaurants) and a city (e.g. Edinburgh) and work on the recommender using only this subset of the data.\n",
    "\n",
    "Please take some time to:\n",
    "- find out what information you will need to feed as input to Spark's ALS\n",
    "- check how this information is available in the dataset\n",
    "- plan how you will tackle this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small version of the Yelp dataset\n",
    "#!wget https://s3.us-west-2.amazonaws.com/dsr-spark-appliedml/yelp_dataset_small.tar.gz\n",
    "#!tar -xvzf yelp_dataset_small.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext('local[*]')\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Data\n",
    "\n",
    "- Load the file ***yelp_academic_dataset_business.json*** and select the following columns:\n",
    "    - business_id\n",
    "    - name\n",
    "    - city\n",
    "    - stars\n",
    "    - categories\n",
    "    - address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = sqlc.read.json('yelp_dataset_small/yelp_academic_dataset_business.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+-----+\n",
      "|             address|         business_id|          categories|      city|                name|stars|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+-----+\n",
      "|227 E Baseline Rd...|0DI8Dt2PJp07XkVvI...|[Tobacco Shops, N...|     Tempe|   Innovative Vapors|  4.5|\n",
      "|495 S Grand Centr...|LTlCaCGZE14GuaUXU...|[Caterers, Grocer...| Las Vegas|       Cut and Taste|  5.0|\n",
      "|  979 Bloor Street W|EDqCEAGXVGCH4FJXg...|[Restaurants, Piz...|   Toronto|         Pizza Pizza|  2.5|\n",
      "|7014 Steubenville...|cnGIivYRLxpF7tBVR...|[Hair Removal, Be...|   Oakdale| Plush Salon and Spa|  4.0|\n",
      "|   321 Jarvis Street|cdk-qqJ71q6P7TJTw...|[Hotels & Travel,...|   Toronto|         Comfort Inn|  3.0|\n",
      "|30 Gibson Drive, ...|Q9rsaUiQ-A3NdEAlo...|[Nail Salons, Bea...|   Markham|         A Plus Nail|  2.5|\n",
      "|10875 N Frankloyd...|Cu4_Fheh7IrzGiK-P...|[Baby Gear & Furn...|Scottsdale|      Boomerang Baby|  3.5|\n",
      "|11072 No Frank Ll...|GDnbt3isfhd57T1Qq...|[Tex-Mex, Mexican...|Scottsdale|           Taco Bell|  2.5|\n",
      "|11000 North 115th...|qwAHit4Tuj1zpO7Cx...|[Local Services, ...|Scottsdale|CubeSmart Self St...|  4.5|\n",
      "|         Hauptstr. 1|Nbr0kbtIrVlEcKIZo...|    [Food, Bakeries]| Stuttgart|     Sehne Backwaren|  3.5|\n",
      "|    2553 Wigwam Pkwy|MFneYHieJ_lnjMeFU...|[Nail Salons, Hai...| Henderson|      Revv Illusions|  5.0|\n",
      "|1500 N Green Vall...|42romV8altAeuZuP2...|[Hawaiian, Restau...| Henderson|  Ohana Hawaiian BBQ|  4.0|\n",
      "|5670 Sherbrooke R...|iaunX_af5M5lfT2eE...|[Shopping, Bookst...|  Montréal|Encore Books and ...|  5.0|\n",
      "| 2525 W Carefree Hwy|Tc24GX9-ZPr4_SHU0...|[General Dentistr...|   Phoenix|Canyon Ridge Endo...|  4.0|\n",
      "|6801 North Lake M...|6EvETd9FVPJfhT_6A...|[Leather Goods, F...| Charlotte|        Fossil Store|  3.5|\n",
      "|2500 Winston Park...|DSWsjtAfLYw9a4MTz...|[Mobile Phones, C...|  Oakville|            Best Buy|  3.0|\n",
      "|                    |SbfEPi-iR4ntf3wRQ...|[Animal Shelters,...|Scottsdale|Woofs, Wiggles N ...|  4.5|\n",
      "| 1000 Queen Street W|YCsLfBVdLFeN2Necw...|[Shoe Stores, Fas...|   Toronto|              Stussy|  2.0|\n",
      "|   610 John Nolen Dr|O_4OTnw48ULP5uZh6...|[Hotels & Travel,...|   Madison|Holiday Inn Expre...|  4.5|\n",
      "|330 S Gilbert Rd,...|CE0dABv9sfrXjDIJu...|    [Food, Bakeries]|      Mesa|Graceful Cake Cre...|  5.0|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_business = df_business.select('business_id', \n",
    "                                 'name', \n",
    "                                 'city', \n",
    "                                 'stars', \n",
    "                                 'categories', \n",
    "                                 'address')\n",
    "df_business.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a business category\n",
    "\n",
    "- Define a regular Python function that takes a list of categories and returns 1 if a category of your choice (for instance, 'Restaurants') is contained in the list of categories or 0 otherwise\n",
    "- Using the Python function, define a Spark's User Defined Function (UDF) with an IntegerType return\n",
    "- Using the UDF, filter the businesses that belong to the category you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'pyspark.sql.udf.UserDefinedFunction'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def is_restaurant(categories):\n",
    "    return ('Restaurants' in categories) * 1 if categories is not None else 0\n",
    "\n",
    "udf = UserDefinedFunction(is_restaurant, returnType='Integer')\n",
    "\n",
    "df_restaurants = df_business.filter(udf(df_business.categories) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+--------------------+-----+\n",
      "|             address|         business_id|          categories|        city|                name|stars|\n",
      "+--------------------+--------------------+--------------------+------------+--------------------+-----+\n",
      "|  979 Bloor Street W|EDqCEAGXVGCH4FJXg...|[Restaurants, Piz...|     Toronto|         Pizza Pizza|  2.5|\n",
      "|11072 No Frank Ll...|GDnbt3isfhd57T1Qq...|[Tex-Mex, Mexican...|  Scottsdale|           Taco Bell|  2.5|\n",
      "|1500 N Green Vall...|42romV8altAeuZuP2...|[Hawaiian, Restau...|   Henderson|  Ohana Hawaiian BBQ|  4.0|\n",
      "|1052 Lionel-Dauna...|DNyYOxVAfu0oUcPNL...|[Restaurants, Cafes]|Boucherville|         Chez Lionel|  3.5|\n",
      "|2000 Mansfield St...|a1Ba6XeIOP48e64YF...|[Sandwiches, Brea...|    Montréal|             La Prep|  4.0|\n",
      "|123 Front St, Uni...|826djy6K_9Fp0ptqJ...|[Fast Food, Mexic...|     Toronto|Chipotle Mexican ...|  3.5|\n",
      "|      5646 W Bell Rd|Mi5uhdFB9OJteXPd0...|[Restaurants, Ita...|    Glendale|Carrabba's Italia...|  3.5|\n",
      "|      2269 Kresge Dr|Uxh0fXFH_QQBivRnI...|[Restaurants, Mex...|     Amherst|         Don Tequila|  3.5|\n",
      "|  1220 S Central Ave|YPavuOh2XsnRbLfl0...|[Restaurants, Waf...|     Phoenix|Lo-Lo's Chicken &...|  4.0|\n",
      "| 4811 S Rainbow Blvd|saWZO6hB4B8P-mIzS...|[Persian/Iranian,...|   Las Vegas|        Kabob Palace|  2.5|\n",
      "|13801 Indian Trai...|vJ-KyaojDQj1R2BNr...|[American (Tradit...|Indian Trail|  East 74 Restaurant|  3.0|\n",
      "|3732 Darrow Rd Ste 5|hovoWva_UjbnyLWEb...| [Thai, Restaurants]|        Stow|        Thai Gourmet|  3.5|\n",
      "| 2014 Queen Street E|OLG7Gou8kmTLxogtJ...|[Tex-Mex, Restaur...|     Toronto|              Z-Teca|  1.5|\n",
      "|   1700 N Chester St|8Tq8l82FtDyHelc6C...|[Breakfast & Brun...|    Gastonia|       Pancake House|  3.5|\n",
      "|      337 N Shore Dr|93otbGHE0s0m-lU1o...|[American (New), ...|  Pittsburgh|          Rivertowne|  3.0|\n",
      "|   2816 Markham Road|L_thK7r3K_h5M4tV7...|[Diners, Italian,...|     Toronto|Honey B Hives Res...|  3.5|\n",
      "|         1714 S Blvd|LIvXzdMJrIb6IY5Y3...|[American (New), ...|   Charlotte|     Nan and Byron's|  3.5|\n",
      "|7A Castle Street,...|NsarUMMMPOlMBb6K0...|[Food, Fast Food,...|   Edinburgh|      Juice Almighty|  4.5|\n",
      "|   3557 W Dunlap Ave|F53MSa5SYzO9BG8c_...|[Vietnamese, Rest...|     Phoenix|            Pho Viet|  2.5|\n",
      "| 3567 W Northern Ave|hEcn9k6ONd5n2mq0l...|[American (Tradit...|     Phoenix|     JB's Restaurant|  3.5|\n",
      "+--------------------+--------------------+--------------------+------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_restaurants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The UDF approach works just fine, but there is a more straightforward way to perform the same operation\n",
    "    - hint: look at ***array_contains*** SQL function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# you can overwrite the former df_restaurants\n",
    "df_restaurants = df_business.filter(F.array_contains(df_business.categories, \"Restaurants\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+--------------------+-----+\n",
      "|             address|         business_id|          categories|        city|                name|stars|\n",
      "+--------------------+--------------------+--------------------+------------+--------------------+-----+\n",
      "|  979 Bloor Street W|EDqCEAGXVGCH4FJXg...|[Restaurants, Piz...|     Toronto|         Pizza Pizza|  2.5|\n",
      "|11072 No Frank Ll...|GDnbt3isfhd57T1Qq...|[Tex-Mex, Mexican...|  Scottsdale|           Taco Bell|  2.5|\n",
      "|1500 N Green Vall...|42romV8altAeuZuP2...|[Hawaiian, Restau...|   Henderson|  Ohana Hawaiian BBQ|  4.0|\n",
      "|1052 Lionel-Dauna...|DNyYOxVAfu0oUcPNL...|[Restaurants, Cafes]|Boucherville|         Chez Lionel|  3.5|\n",
      "|2000 Mansfield St...|a1Ba6XeIOP48e64YF...|[Sandwiches, Brea...|    Montréal|             La Prep|  4.0|\n",
      "|123 Front St, Uni...|826djy6K_9Fp0ptqJ...|[Fast Food, Mexic...|     Toronto|Chipotle Mexican ...|  3.5|\n",
      "|      5646 W Bell Rd|Mi5uhdFB9OJteXPd0...|[Restaurants, Ita...|    Glendale|Carrabba's Italia...|  3.5|\n",
      "|      2269 Kresge Dr|Uxh0fXFH_QQBivRnI...|[Restaurants, Mex...|     Amherst|         Don Tequila|  3.5|\n",
      "|  1220 S Central Ave|YPavuOh2XsnRbLfl0...|[Restaurants, Waf...|     Phoenix|Lo-Lo's Chicken &...|  4.0|\n",
      "| 4811 S Rainbow Blvd|saWZO6hB4B8P-mIzS...|[Persian/Iranian,...|   Las Vegas|        Kabob Palace|  2.5|\n",
      "|13801 Indian Trai...|vJ-KyaojDQj1R2BNr...|[American (Tradit...|Indian Trail|  East 74 Restaurant|  3.0|\n",
      "|3732 Darrow Rd Ste 5|hovoWva_UjbnyLWEb...| [Thai, Restaurants]|        Stow|        Thai Gourmet|  3.5|\n",
      "| 2014 Queen Street E|OLG7Gou8kmTLxogtJ...|[Tex-Mex, Restaur...|     Toronto|              Z-Teca|  1.5|\n",
      "|   1700 N Chester St|8Tq8l82FtDyHelc6C...|[Breakfast & Brun...|    Gastonia|       Pancake House|  3.5|\n",
      "|      337 N Shore Dr|93otbGHE0s0m-lU1o...|[American (New), ...|  Pittsburgh|          Rivertowne|  3.0|\n",
      "|   2816 Markham Road|L_thK7r3K_h5M4tV7...|[Diners, Italian,...|     Toronto|Honey B Hives Res...|  3.5|\n",
      "|         1714 S Blvd|LIvXzdMJrIb6IY5Y3...|[American (New), ...|   Charlotte|     Nan and Byron's|  3.5|\n",
      "|7A Castle Street,...|NsarUMMMPOlMBb6K0...|[Food, Fast Food,...|   Edinburgh|      Juice Almighty|  4.5|\n",
      "|   3557 W Dunlap Ave|F53MSa5SYzO9BG8c_...|[Vietnamese, Rest...|     Phoenix|            Pho Viet|  2.5|\n",
      "| 3567 W Northern Ave|hEcn9k6ONd5n2mq0l...|[American (Tradit...|     Phoenix|     JB's Restaurant|  3.5|\n",
      "+--------------------+--------------------+--------------------+------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_restaurants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a city\n",
    "- Having filtered by the business category, now it is time to filter by the city (for instance, Edinburgh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_restaurants = df_restaurants.filter(df_restaurants.city == 'Edinburgh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------+--------------------+-----+\n",
      "|             address|         business_id|          categories|     city|                name|stars|\n",
      "+--------------------+--------------------+--------------------+---------+--------------------+-----+\n",
      "|7A Castle Street,...|NsarUMMMPOlMBb6K0...|[Food, Fast Food,...|Edinburgh|      Juice Almighty|  4.5|\n",
      "|   1 Sighthill Court|m5CY1jy3dvBw8J4ii...|[Nightlife, Bars,...|Edinburgh|            Crofters|  3.5|\n",
      "|   119 Dundee Street|raixiox15brAXfTUb...|[Fast Food, Halal...|Edinburgh|          Spicy Bite|  3.5|\n",
      "|Odeon Cinema, 118...|ynjAEdXdIw7JF153u...|[Italian, Restaur...|Edinburgh|      Croma Pizzeria|  3.5|\n",
      "|        123b High St|aCul-vH-5hCxX6XZs...|[Restaurants, Bri...|Edinburgh|Dubh Prais Restau...|  4.5|\n",
      "|76 Commercial Street|mwO4cm8qs32djjve5...|[Restaurants, Chi...|Edinburgh|     Chop Chop Leith|  4.0|\n",
      "|226 Oxgangs Road ...|CZRba6sPzqpGwWpz5...|[Restaurants, Fis...|Edinburgh|            Chip Inn|  3.5|\n",
      "|  36-38 Blair Street|SBZEyV_T_heCgE0f6...|[Coffee & Tea, Ba...|Edinburgh|       Cafe Voltaire|  4.0|\n",
      "| 15-16 Market Street|MuVIpRAEkNfRnmhsP...|[Gastropubs, Pubs...|Edinburgh|           The Doric|  4.0|\n",
      "|Waldorf Astoria -...|_ktENpyAriCyBBrQa...|[French, Restaura...|Edinburgh|The Pompadour by ...|  4.5|\n",
      "|      Princes Street|pvuqXq2ricB1klqZx...|[Restaurants, Hot...|Edinburgh|       Mercure Hotel|  3.5|\n",
      "|    42 Dundas Street|K8s-oKzrcLwBcNDOs...|[Restaurants, Cafes]|Edinburgh|            Ravenous|  4.5|\n",
      "|   97 Hanover Street|4ivcXYJIPSHkwGwu5...|[Tapas Bars, Rest...|Edinburgh|                Tapa|  4.5|\n",
      "|         2 Dundee St|iTezzhTI7NTrMkn6v...|[Pizza, Restaurants]|Edinburgh|           Pizza Hut|  3.5|\n",
      "| 14 Broughton Street|VgoTwOJR1RwW8Jp_x...|[Sushi Bars, Japa...|Edinburgh|              Bonsai|  3.5|\n",
      "|         50 Potterow|ksP8426d2qN-mg-JD...|[Food, Imported F...|Edinburgh|The Original Mosq...|  4.0|\n",
      "|     222 Gorgie Road|gLu_vZr0xLyeBWz-v...|[Restaurants, Fas...|Edinburgh|    Baguette Express|  4.5|\n",
      "|            Annfield|gm83csiJtk7RP_jco...|[Restaurants, Fas...|Edinburgh|         Ocean Spice|  4.5|\n",
      "| 39 Broughton Street|xjBvbDmbsA0VX-vA5...|[Restaurants, Bar...|Edinburgh|             Treacle|  4.0|\n",
      "|97-101 Fountainbr...|kUVgPJ7eH4bciKmmn...|[Malaysian, Resta...|Edinburgh|         Kampung Ali|  4.0|\n",
      "+--------------------+--------------------+--------------------+---------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_restaurants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating numeric IDs\n",
    "- If you haven't done it yet, take one sample from your already filtered DataFrame and notice that the ***business_id*** contains an alphanumeric value - this is not good for Spark's ALS implementation, which requires IDs for items (in our case, businesses) and users to be numeric\n",
    "- Use a ***StringIndexer*** to create a new column ***business_idn*** from the conversion of business_id into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"business_id\", outputCol=\"business_idn\", handleInvalid='error')\n",
    "\n",
    "df_city_restaurants = stringIndexer.fit(df_city_restaurants).transform(df_city_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(address='7A Castle Street, Corstorphine', business_id='NsarUMMMPOlMBb6K04x6hw', categories=['Food', 'Fast Food', 'Restaurants', 'Juice Bars & Smoothies'], city='Edinburgh', name='Juice Almighty', stars=4.5, business_idn=24.0)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_restaurants.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[address: string, business_id: string, categories: array<string>, city: string, name: string, stars: double, business_idn: double]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_restaurants.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Data\n",
    "\n",
    "- Load the file ***yelp_academic_dataset_review.json*** and select the following columns:\n",
    "    - user_id\n",
    "    - business-id\n",
    "    - stars\n",
    "    - date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = sqlc.read.json('yelp_dataset_small/yelp_academic_dataset_review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------------+\n",
      "|         business_id|      date|stars|             user_id|\n",
      "+--------------------+----------+-----+--------------------+\n",
      "|2aFiy99vNLklCx3T_...|2011-10-10|    5|KpkOkG6RIf4Ra25Lh...|\n",
      "|2aFiy99vNLklCx3T_...|2010-12-29|    5|bQ7fQq1otn9hKX-gX...|\n",
      "|2aFiy99vNLklCx3T_...|2011-04-29|    5|r1NUhdNmL6yU9Bn-Y...|\n",
      "|2LfIuF3_sX6uwe-IR...|2014-07-14|    5|aW3ix1KNZAvoM8q-W...|\n",
      "|2LfIuF3_sX6uwe-IR...|2014-01-15|    4|YOo-Cip8HqvKp_p9n...|\n",
      "|2LfIuF3_sX6uwe-IR...|2013-04-28|    5|bgl3j8yJcRO-00NkU...|\n",
      "|2LfIuF3_sX6uwe-IR...|2014-10-12|    4|CWKF9de-nskLYEqDD...|\n",
      "|2LfIuF3_sX6uwe-IR...|2012-09-18|    5|GJ7PTY7huYORFKKg3...|\n",
      "|2LfIuF3_sX6uwe-IR...|2015-10-11|    5|rxqp9eXZj1jYTn0UI...|\n",
      "|2LfIuF3_sX6uwe-IR...|2015-04-05|    5|UU0nHQtHPMAfLidk8...|\n",
      "|2LfIuF3_sX6uwe-IR...|2014-07-08|    1|A_Hyfk3FcwFVIk1CQ...|\n",
      "|2LfIuF3_sX6uwe-IR...|2014-08-23|    5|OvD92wp0-uuFoGLBy...|\n",
      "|2LfIuF3_sX6uwe-IR...|2015-01-13|    4|5NDk-q5mv8PIDvz83...|\n",
      "|2LfIuF3_sX6uwe-IR...|2014-08-05|    5|AziQIgYIAY6uVw1k8...|\n",
      "|2LfIuF3_sX6uwe-IR...|2014-12-22|    5|A65IYKs3FwsyyJH20...|\n",
      "|2LfIuF3_sX6uwe-IR...|2013-11-17|    1|wnzfuir72IZFg5RAP...|\n",
      "|2LfIuF3_sX6uwe-IR...|2013-07-10|    4|2N9wrn5A37aOXDrBP...|\n",
      "|2LfIuF3_sX6uwe-IR...|2014-08-23|    5|oKZiDLm0D1PIm1-vR...|\n",
      "|0czfEgv9KAD4VlIa7...|2010-02-16|    1|XMRkVYpQAZpWOpia5...|\n",
      "|0czfEgv9KAD4VlIa7...|2009-04-10|    5|jhhHm3Vk9ZlP21WdY...|\n",
      "+--------------------+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews = df_reviews.select('user_id', \n",
    "                               'business_id', \n",
    "                               'stars', \n",
    "                               'date')\n",
    "df_reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping reviews for the chosen city only\n",
    "\n",
    "- You are only interested in reviews of businesses you kept after filtering for category and city - how to filter out everything else? (hint: take a look at the ***join*** operation of DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_reviews = (df_reviews\n",
    "    .join(df_city_restaurants, df_reviews.business_id == df_city_restaurants.business_id)\n",
    "    .select(df_reviews.user_id, df_city_restaurants.business_idn, df_reviews.stars, \n",
    "           df_reviews.date, df_city_restaurants.city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+----------+---------+\n",
      "|             user_id|business_idn|stars|      date|     city|\n",
      "+--------------------+------------+-----+----------+---------+\n",
      "|VRVCKQhYDCkzaEDce...|      1208.0|    5|2008-07-06|Edinburgh|\n",
      "|SxV1Jq7UANuSYpn42...|      1208.0|    1|2010-04-15|Edinburgh|\n",
      "|soDF6mePh1SuNZI3r...|      1208.0|    3|2015-03-10|Edinburgh|\n",
      "|LURC3E0DoXYgN9aYT...|       684.0|    2|2012-02-04|Edinburgh|\n",
      "|W1Nl6_R7amuZ6NStX...|       684.0|    1|2011-02-24|Edinburgh|\n",
      "|hutJzKEYHuVq6CP-X...|       684.0|    2|2010-08-05|Edinburgh|\n",
      "|fcMTpwfLS9F5DWTql...|      1351.0|    4|2012-08-23|Edinburgh|\n",
      "|yuFHrb8YQtVuzu0eE...|      1351.0|    4|2013-04-19|Edinburgh|\n",
      "|yfXqZkU5iXE07GSHz...|       214.0|    4|2010-06-16|Edinburgh|\n",
      "|v_lWueG7V_vul4E8r...|       214.0|    4|2010-07-02|Edinburgh|\n",
      "|LURC3E0DoXYgN9aYT...|       214.0|    4|2012-05-17|Edinburgh|\n",
      "|IY8cvV2SQuhJjKhRW...|       214.0|    4|2010-07-17|Edinburgh|\n",
      "|y4uy-FJ9UDamTVJ6Y...|       214.0|    2|2015-06-25|Edinburgh|\n",
      "|PbgqvUdjTRh-HoJUq...|       214.0|    4|2015-10-02|Edinburgh|\n",
      "|j8FLszyM98srKnasj...|       214.0|    4|2016-03-15|Edinburgh|\n",
      "|hwbBTEwHbR2k6kk-T...|       214.0|    4|2015-11-18|Edinburgh|\n",
      "|Cy1PV2TdYeDFA16mU...|       214.0|    5|2010-07-22|Edinburgh|\n",
      "|X42QbBZjG0llxV7Ea...|        81.0|    5|2008-04-28|Edinburgh|\n",
      "|3Tk2Fc6mNlBww17t2...|        81.0|    3|2009-02-12|Edinburgh|\n",
      "|8unLlv-UgMcasYJr2...|        81.0|    4|2007-10-10|Edinburgh|\n",
      "+--------------------+------------+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating numeric IDs\n",
    "\n",
    "- As it happened with the ***business_id***, you also need to convert ***user_id*** into a numeric value - once again, use a ***StringIndexer*** to create a new column named ***user_idn*** containing the result of the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_idn\", handleInvalid='error')\n",
    "\n",
    "df_city_reviews = stringIndexer.fit(df_city_reviews).transform(df_city_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_reviews = df_city_reviews.drop('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+----------+---------+--------+\n",
      "|             user_id|business_idn|stars|      date|     city|user_idn|\n",
      "+--------------------+------------+-----+----------+---------+--------+\n",
      "|VRVCKQhYDCkzaEDce...|      1208.0|    5|2008-07-06|Edinburgh|    63.0|\n",
      "|SxV1Jq7UANuSYpn42...|      1208.0|    1|2010-04-15|Edinburgh|     6.0|\n",
      "|soDF6mePh1SuNZI3r...|      1208.0|    3|2015-03-10|Edinburgh|   433.0|\n",
      "|LURC3E0DoXYgN9aYT...|       684.0|    2|2012-02-04|Edinburgh|     2.0|\n",
      "|W1Nl6_R7amuZ6NStX...|       684.0|    1|2011-02-24|Edinburgh|    91.0|\n",
      "|hutJzKEYHuVq6CP-X...|       684.0|    2|2010-08-05|Edinburgh|    54.0|\n",
      "|fcMTpwfLS9F5DWTql...|      1351.0|    4|2012-08-23|Edinburgh|    15.0|\n",
      "|yuFHrb8YQtVuzu0eE...|      1351.0|    4|2013-04-19|Edinburgh|    34.0|\n",
      "|yfXqZkU5iXE07GSHz...|       214.0|    4|2010-06-16|Edinburgh|     3.0|\n",
      "|v_lWueG7V_vul4E8r...|       214.0|    4|2010-07-02|Edinburgh|    87.0|\n",
      "|LURC3E0DoXYgN9aYT...|       214.0|    4|2012-05-17|Edinburgh|     2.0|\n",
      "|IY8cvV2SQuhJjKhRW...|       214.0|    4|2010-07-17|Edinburgh|    28.0|\n",
      "|y4uy-FJ9UDamTVJ6Y...|       214.0|    2|2015-06-25|Edinburgh|  1581.0|\n",
      "|PbgqvUdjTRh-HoJUq...|       214.0|    4|2015-10-02|Edinburgh|    76.0|\n",
      "|j8FLszyM98srKnasj...|       214.0|    4|2016-03-15|Edinburgh|   767.0|\n",
      "|hwbBTEwHbR2k6kk-T...|       214.0|    4|2015-11-18|Edinburgh|   589.0|\n",
      "|Cy1PV2TdYeDFA16mU...|       214.0|    5|2010-07-22|Edinburgh|     4.0|\n",
      "|X42QbBZjG0llxV7Ea...|        81.0|    5|2008-04-28|Edinburgh|   652.0|\n",
      "|3Tk2Fc6mNlBww17t2...|        81.0|    3|2009-02-12|Edinburgh|  2143.0|\n",
      "|8unLlv-UgMcasYJr2...|        81.0|    4|2007-10-10|Edinburgh|   329.0|\n",
      "+--------------------+------------+-----+----------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, business_idn: double, stars: bigint, date: string, city: string, user_idn: double]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_reviews.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a sequential number to the user's reviews\n",
    "\n",
    "- Now add a ***sequential number*** to the user's reviews, that is, for each user, order his/her reviews by date (multiple reviews on the same date can be randomly ordered) and number them (hint: check ***window functions***)\n",
    "- This sequential number will be useful later to perform a time-wise split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.partitionBy('user_idn').orderBy(\"date\")\n",
    "\n",
    "df_city_reviews = df_city_reviews.withColumn('sequential_number', F.row_number().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+---------+--------+-----------------+----------+\n",
      "|business_idn|stars|      date|     city|user_idn|sequential_number|no_reviews|\n",
      "+------------+-----+----------+---------+--------+-----------------+----------+\n",
      "|       327.0|    5|2011-04-20|Edinburgh|   299.0|                1|         2|\n",
      "|       144.0|    5|2011-04-20|Edinburgh|   299.0|                2|         2|\n",
      "|      1047.0|    2|2011-05-02|Edinburgh|   299.0|                3|         4|\n",
      "|       228.0|    4|2011-05-02|Edinburgh|   299.0|                4|         4|\n",
      "|       942.0|    5|2011-05-19|Edinburgh|   299.0|                5|         5|\n",
      "|        51.0|    5|2011-05-25|Edinburgh|   299.0|                6|         6|\n",
      "|       850.0|    3|2011-05-30|Edinburgh|   299.0|                7|         8|\n",
      "|       703.0|    4|2011-05-30|Edinburgh|   299.0|                8|         8|\n",
      "|      1336.0|    5|2011-06-01|Edinburgh|   299.0|                9|         9|\n",
      "|       460.0|    5|2011-06-09|Edinburgh|   299.0|               10|        10|\n",
      "|        75.0|    5|2011-06-10|Edinburgh|   299.0|               11|        11|\n",
      "|       757.0|    4|2011-07-26|Edinburgh|   299.0|               12|        12|\n",
      "|       827.0|    4|2008-08-01|Edinburgh|   305.0|                1|         1|\n",
      "|       891.0|    5|2008-08-08|Edinburgh|   305.0|                2|         2|\n",
      "|       718.0|    3|2008-08-10|Edinburgh|   305.0|                3|         3|\n",
      "|       745.0|    5|2008-09-23|Edinburgh|   305.0|                4|         4|\n",
      "|       385.0|    4|2008-09-30|Edinburgh|   305.0|                5|         7|\n",
      "|      1135.0|    4|2008-09-30|Edinburgh|   305.0|                6|         7|\n",
      "|       869.0|    2|2008-09-30|Edinburgh|   305.0|                7|         7|\n",
      "|       162.0|    3|2008-11-15|Edinburgh|   305.0|                8|        11|\n",
      "+------------+-----+----------+---------+--------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting reviews to keep only users with more than 4 reviews\n",
    "\n",
    "- Some users had rated only 1 or a few businesses - this would pose as a problem to make recommendations - so you would want to keep only users who had rated more than 4 reviews, for instance\n",
    "- Find the ***total number of reviews*** for each user and then filter them using this information (hint: again, you can use a ***window function***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('user_idn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_city_reviews.withColumn('no_reviews', \n",
    "                                             F.max('sequential_number').over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+---------+--------+-----------------+----------+\n",
      "|business_idn|stars|      date|     city|user_idn|sequential_number|no_reviews|\n",
      "+------------+-----+----------+---------+--------+-----------------+----------+\n",
      "|       327.0|    5|2011-04-20|Edinburgh|   299.0|                1|        12|\n",
      "|       144.0|    5|2011-04-20|Edinburgh|   299.0|                2|        12|\n",
      "|      1047.0|    2|2011-05-02|Edinburgh|   299.0|                3|        12|\n",
      "|       228.0|    4|2011-05-02|Edinburgh|   299.0|                4|        12|\n",
      "|       942.0|    5|2011-05-19|Edinburgh|   299.0|                5|        12|\n",
      "|        51.0|    5|2011-05-25|Edinburgh|   299.0|                6|        12|\n",
      "|       850.0|    3|2011-05-30|Edinburgh|   299.0|                7|        12|\n",
      "|       703.0|    4|2011-05-30|Edinburgh|   299.0|                8|        12|\n",
      "|      1336.0|    5|2011-06-01|Edinburgh|   299.0|                9|        12|\n",
      "|       460.0|    5|2011-06-09|Edinburgh|   299.0|               10|        12|\n",
      "|        75.0|    5|2011-06-10|Edinburgh|   299.0|               11|        12|\n",
      "|       757.0|    4|2011-07-26|Edinburgh|   299.0|               12|        12|\n",
      "|       827.0|    4|2008-08-01|Edinburgh|   305.0|                1|        12|\n",
      "|       891.0|    5|2008-08-08|Edinburgh|   305.0|                2|        12|\n",
      "|       718.0|    3|2008-08-10|Edinburgh|   305.0|                3|        12|\n",
      "|       745.0|    5|2008-09-23|Edinburgh|   305.0|                4|        12|\n",
      "|       385.0|    4|2008-09-30|Edinburgh|   305.0|                5|        12|\n",
      "|      1135.0|    4|2008-09-30|Edinburgh|   305.0|                6|        12|\n",
      "|       869.0|    2|2008-09-30|Edinburgh|   305.0|                7|        12|\n",
      "|       162.0|    3|2008-11-15|Edinburgh|   305.0|                8|        12|\n",
      "+------------+-----+----------+---------+--------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_idn: double, stars: bigint, date: string, city: string, user_idn: double, sequential_number: int, no_reviews: int]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating mean rating by user\n",
    "\n",
    "- Now you can calculate the mean rating by user and make it into a dictionary where the key is the ***user_id*** (hint: look at ***rdd*** method of DataFrames and ***collectAsMap*** method of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------------+\n",
      "|user_idn|avg(user_idn)|        avg(stars)|\n",
      "+--------+-------------+------------------+\n",
      "|   299.0|        299.0| 4.333333333333333|\n",
      "|   305.0|        305.0|3.9166666666666665|\n",
      "|   496.0|        496.0| 2.857142857142857|\n",
      "|   558.0|        558.0|3.1666666666666665|\n",
      "|   596.0|        596.0| 4.166666666666667|\n",
      "|   692.0|        692.0|               3.0|\n",
      "|   769.0|        769.0|               3.8|\n",
      "|   934.0|        934.0|              3.75|\n",
      "|  1051.0|       1051.0|               4.5|\n",
      "|  1761.0|       1761.0|               4.5|\n",
      "|  2734.0|       2734.0|               3.0|\n",
      "|  2815.0|       2815.0|               1.0|\n",
      "|  2862.0|       2862.0|               5.0|\n",
      "|  3597.0|       3597.0|               5.0|\n",
      "|  3901.0|       3901.0|               5.0|\n",
      "|  3980.0|       3980.0|               4.0|\n",
      "|  4066.0|       4066.0|               5.0|\n",
      "|  4142.0|       4142.0|               5.0|\n",
      "|  4800.0|       4800.0|               5.0|\n",
      "|  5360.0|       5360.0|               5.0|\n",
      "+--------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected.select('user_idn', 'stars').groupby('user_idn').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_user_means = df_selected.select('user_idn', 'stars') \\\n",
    "                             .groupby('user_idn') \\\n",
    "                             .mean() \\\n",
    "                             .drop('avg(user_idn)') \\\n",
    "                             .rdd \\\n",
    "                             .collectAsMap() \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.333333333333333"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_user_means[299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering rating by user\n",
    "\n",
    "- The dictionary containing mean ratings by user can be seen as a ***lookup table*** - what is the appropriate way of dealing with those in Spark?\n",
    "- Once you have figured this out, define a regular Python function that takes two arguments - ***user_id*** (String) and ***rating*** (String, which you will need to convert to float inside the function) - and returns the result of subtracting the mean rating of the user from the rating parameter\n",
    "- Using the Python function, define a Spark's User Defined Function (UDF) with a DoubleType return\n",
    "- Using the UDF, create a column in your DataFrame with the centered ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "lookup_user_means = ...\n",
    "\n",
    "def zero_mean(user_id, rating):\n",
    "    pass\n",
    "\n",
    "df_centered = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once again, the UDF approach is not the most \"Sparkonic\" way of handling this - can you perform the same operation using only functions from ***pyspark.sql.functions*** (which was imported earlier as F)?\n",
    "    - hint: you'll need ***Window functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can overwrite df_centered\n",
    "window = Window.partitionBy('user_idn')\n",
    "df_centered = (df_selected\n",
    "               .withColumn('avg_stars', F.avg('stars').over(window))\n",
    "               .withColumn('stars', F.expr('stars - avg_stars'))\n",
    "               .drop('avg_stars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+---------+--------+-----------------+----------+\n",
      "|business_idn|               stars|      date|     city|user_idn|sequential_number|no_reviews|\n",
      "+------------+--------------------+----------+---------+--------+-----------------+----------+\n",
      "|       327.0|   0.666666666666667|2011-04-20|Edinburgh|   299.0|                1|        12|\n",
      "|       144.0|   0.666666666666667|2011-04-20|Edinburgh|   299.0|                2|        12|\n",
      "|      1047.0|  -2.333333333333333|2011-05-02|Edinburgh|   299.0|                3|        12|\n",
      "|       228.0|-0.33333333333333304|2011-05-02|Edinburgh|   299.0|                4|        12|\n",
      "|       942.0|   0.666666666666667|2011-05-19|Edinburgh|   299.0|                5|        12|\n",
      "|        51.0|   0.666666666666667|2011-05-25|Edinburgh|   299.0|                6|        12|\n",
      "|       850.0|  -1.333333333333333|2011-05-30|Edinburgh|   299.0|                7|        12|\n",
      "|       703.0|-0.33333333333333304|2011-05-30|Edinburgh|   299.0|                8|        12|\n",
      "|      1336.0|   0.666666666666667|2011-06-01|Edinburgh|   299.0|                9|        12|\n",
      "|       460.0|   0.666666666666667|2011-06-09|Edinburgh|   299.0|               10|        12|\n",
      "|        75.0|   0.666666666666667|2011-06-10|Edinburgh|   299.0|               11|        12|\n",
      "|       757.0|-0.33333333333333304|2011-07-26|Edinburgh|   299.0|               12|        12|\n",
      "|       827.0| 0.08333333333333348|2008-08-01|Edinburgh|   305.0|                1|        12|\n",
      "|       891.0|  1.0833333333333335|2008-08-08|Edinburgh|   305.0|                2|        12|\n",
      "|       718.0| -0.9166666666666665|2008-08-10|Edinburgh|   305.0|                3|        12|\n",
      "|       745.0|  1.0833333333333335|2008-09-23|Edinburgh|   305.0|                4|        12|\n",
      "|       385.0| 0.08333333333333348|2008-09-30|Edinburgh|   305.0|                5|        12|\n",
      "|      1135.0| 0.08333333333333348|2008-09-30|Edinburgh|   305.0|                6|        12|\n",
      "|       869.0| -1.9166666666666665|2008-09-30|Edinburgh|   305.0|                7|        12|\n",
      "|       162.0| -0.9166666666666665|2008-11-15|Edinburgh|   305.0|                8|        12|\n",
      "+------------+--------------------+----------+---------+--------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_centered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training and test sets by time\n",
    "\n",
    "- In recommender systems, it is common practice to do the training/test split timewise, that is, the test set is composed of the latest reviews\n",
    "- First, filter only those reviews which have a sequential number smaller than the ***total number of reviews***, by user: this is your training set\n",
    "- Then, filter only those reviews which have a sequential number identical to the ***total number of reviews***, by user: this is your test set\n",
    "- Now you can see why you had to add a sequential number to the user's reiews - since some users had done all his/her reviews on the same day, you need to disambiguate them to split the dataset. By doing this, you guarantee your test set will have only 1 review for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df_centered.filter('sequential_number < no_reviews')\n",
    "df_test = df_centered.filter('sequential_number == no_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If using Spark 2.1 (as in the Docker image), you need to filter out \"new\" businesses in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = df_training.select('business_idn').distinct()\n",
    "df_test = df_test.join(businesses, on='business_idn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Least Squares (ALS) Model\n",
    "\n",
    "- This is the recommender itself - the ALS uses a iterative approach to find the underlying factors that yield the user/item rating matrix\n",
    "- It takes as input a DataFrame with three columns, representing:\n",
    "    - userCol: user IDs (numeric - remember the conversion you did)\n",
    "    - itemCol: item IDs (numeric - remember the conversion you did)\n",
    "    - ratingCol: rating (numeric, obviously)\n",
    "    - coldStartStrategy: \"drop\" (if there is unseen data on the test set, meaning a new user/business, drop it) - ***only available from Spark 2.2 on***\n",
    "- Its parameters are:\n",
    "    - rank: the number of factors to consider\n",
    "    - maxIter: the maximum number of iterations to perform\n",
    "    - regParam: the regularization parameter\n",
    "- Use Spark's ALS to fit a model based on your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "als = ALS(rank=10,\n",
    "          maxIter=10, \n",
    "          regParam=0.1, \n",
    "          coldStartStrategy='drop',\n",
    "          userCol='user_idn',\n",
    "          itemCol='business_idn',\n",
    "          ratingCol='stars')\n",
    "df_training = (df_training.select('user_idn', 'business_idn', 'stars')\n",
    "      .withColumnRenamed('user_idn', 'user')\n",
    "      .withColumnRenamed('business_idn', 'item')\n",
    "      .withColumnRenamed('stars', 'rating')\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(df_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the training set\n",
    "\n",
    "- Once the model is trained, make predictions for the training set and use a ***RegressionEvaluator*** to find out the RMSE of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_training.select('user', 'item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_training.join(predictions, on=['user', 'item']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = evaluator.evaluate(df_training.join(predictions, on=['user', 'item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3663683763636205"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the test set\n",
    "\n",
    "- Now, make predictions for the test set and use a ***RegressionEvaluator*** to find out the RMSE of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = (df_test.select('user_idn', 'business_idn', 'stars')\n",
    "      .withColumnRenamed('user_idn', 'user')\n",
    "      .withColumnRenamed('business_idn', 'item')\n",
    "      .withColumnRenamed('stars', 'rating')\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = model.transform(df_test.select('user', 'item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse = evaluator.evaluate(df_test.join(predictions_test, on=['user', 'item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------------------+------------+\n",
      "|  user|  item|              rating|  prediction|\n",
      "+------+------+--------------------+------------+\n",
      "| 283.0| 709.0|  0.0769230769230771|  0.08602212|\n",
      "| 816.0| 491.0| -1.2000000000000002|  0.15445091|\n",
      "| 948.0| 245.0|                 1.5|  0.34028235|\n",
      "| 978.0| 181.0|                0.25| -0.19238092|\n",
      "|1003.0|1130.0|                 1.0| -0.14439525|\n",
      "|1093.0|1227.0|                 2.0|  -1.1584496|\n",
      "|1125.0| 308.0|                 0.0|         0.0|\n",
      "|1272.0| 494.0|  0.6666666666666665| 0.109158695|\n",
      "|1303.0|1300.0| 0.33333333333333304| -0.17399289|\n",
      "|1418.0| 760.0| 0.33333333333333304|  0.23896167|\n",
      "|1540.0| 198.0|-0.33333333333333304|  0.10873921|\n",
      "|1656.0| 815.0|                 0.0|         0.0|\n",
      "|1915.0| 129.0|                 0.5|-0.018868767|\n",
      "|2083.0| 390.0|                -1.5|   -0.766736|\n",
      "|2123.0| 488.0|                 0.0|         0.0|\n",
      "|2129.0| 893.0|                 0.0|         0.0|\n",
      "|2490.0|1102.0|                 0.0|         0.0|\n",
      "|2800.0| 767.0|                 0.0|         NaN|\n",
      "|3139.0| 198.0|                 0.0|         NaN|\n",
      "|3197.0| 287.0|                 0.0|         NaN|\n",
      "+------+------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.join(predictions_test, on=['user', 'item']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+\n",
      "|  user| item| prediction|\n",
      "+------+-----+-----------+\n",
      "|4957.0|767.0|        NaN|\n",
      "|6124.0|767.0|        NaN|\n",
      "|1956.0|767.0|0.053268258|\n",
      "|  40.0|767.0|  0.3869143|\n",
      "|4692.0|767.0|        NaN|\n",
      "|3677.0|767.0|        NaN|\n",
      "|1642.0|767.0|-0.04912223|\n",
      "|5372.0|767.0|        NaN|\n",
      "| 728.0|767.0|  0.2646231|\n",
      "|5443.0|767.0|        NaN|\n",
      "|2516.0|767.0|-0.19701785|\n",
      "|2800.0|767.0|        NaN|\n",
      "|3290.0|767.0|        NaN|\n",
      "|1854.0|767.0| 0.21151784|\n",
      "|3642.0|767.0|        NaN|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test.filter('item == 767').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Now, your model is trained, but how can you use it to make recommendations for a given user?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing business data\n",
    "\n",
    "- It would not make sense to recommend a place the user has already rated, right? So, generate a dictionary where ***user_idn*** is the key and a list of the already rated ***business_idn*** is the value (hint: when aggregating DataFrames, ***collect_list*** is a VERY useful function to turn multiple records into a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "dict_visited_by_user = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Besides, recommending a given business_id also does not help much, right? So you need to organize the business data in a way it can be shown to the user.\n",
    "    - Define a regular Python function that takes one argument ***row*** (Row type) and returns a dictionary where ***business_idn*** is the key and the value is yet another dictionary with relevant fields (for instance: name, address, stars, categories)\n",
    "    - Transform your business DataFrame into an RDD and apply the function you defined - upon collecting, you will end up with a list of dictionaries\n",
    "    - Transform this list of dictionaries into a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rest_to_json(row):\n",
    "    pass\n",
    "\n",
    "rest = ...\n",
    "\n",
    "dict_rest = {k: v for d in rest for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making recommendations for a user\n",
    "\n",
    "- To actually make the recommendations, we need to build an input DataFrame to feed the model\n",
    "    - A DataFrame can be created using the SQL Context and a list of Rows, each containg two columns: user_idn and business_idn - the rating will be computed by the model\n",
    "    - But you only need to have rows for the businesses which were not yet rated by the user - from all businesses, exclude the ones already rated by him/her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "user_idn = 317\n",
    "n_business = len(dict_rest)\n",
    "\n",
    "visited = ...\n",
    "not_visited = ...\n",
    "\n",
    "df_test_user = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, you can use the generated DataFrame to make predictions\n",
    "    - If there are any NA predictions, make sure to turn them into a really bad value (for instance, -5.0) (hint: remember ***na*** method of DataFrames)\n",
    "- Order the predictions and take the ***business_idn*** of the top 5\n",
    "- Finally, use this information to fetch the business data from the dictionary you assembled a couple of steps ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ...\n",
    "\n",
    "top_predictions = ...\n",
    "\n",
    "response = list(map(lambda idn: dict_rest[idn], top_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you finished the exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

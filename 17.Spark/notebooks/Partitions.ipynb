{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: [Partitioning in Apache Spark](https://medium.com/parrot-prediction/partitioning-in-apache-spark-8134ad840b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good summary for the default behavior of partitions can be found [here](https://techmagie.wordpress.com/2015/12/19/understanding-spark-partitioning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = range(0, 10)\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a single core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SparkContext(\"local\") as sc:\n",
    "    rdd = sc.parallelize(nums)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 2 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SparkContext(\"local[2]\") as sc:\n",
    "    rdd = sc.parallelize(nums)\n",
    "    \n",
    "    print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a single core, but specifying manually the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SparkContext(\"local\") as sc:\n",
    "    rdd = sc.parallelize(nums, 15)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 2 cores and specifying a partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SparkContext(\"local[2]\") as sc:\n",
    "    rdd = sc.parallelize(nums) \\\n",
    "        .map(lambda el: (el, el)) \\\n",
    "        .partitionBy(2) \\\n",
    "        .persist()\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "    \n",
    "# partition = partitionFunc(key) % num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "from pyspark.rdd import portable_hash\n",
    "num_partitions = 2\n",
    "for el in nums:\n",
    "    print(\"Element: [{}]: {} % {} = partition {}\".format(el, portable_hash(el), num_partitions, portable_hash(el) % num_partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
    "    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n",
    "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
    "    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n",
    "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a partitioner by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy implementation assuring that data for each country is in one partition\n",
    "def country_partitioner(country):\n",
    "    return hash(country)\n",
    "\n",
    "# Validate results\n",
    "num_partitions = 5\n",
    "print(country_partitioner(\"Poland\") % num_partitions)\n",
    "print(country_partitioner(\"Germany\") % num_partitions)\n",
    "print(country_partitioner(\"United Kingdom\") % num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SparkContext(\"local[2]\") as sc:\n",
    "    rdd = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(4, country_partitioner)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating sum of sales for each partition\n",
    "# Notice that we are getting an iterator.All work is done on one node\n",
    "def sum_sales(iterator):\n",
    "    yield sum(transaction[1]['amount'] for transaction in iterator)\n",
    "    \n",
    "with SparkContext(\"local[2]\") as sc:\n",
    "    by_country = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(3, country_partitioner)\n",
    "    \n",
    "    print(\"Partitions structure: {}\".format(by_country.glom().collect()))\n",
    "    \n",
    "    # Sum sales in each partition\n",
    "    sum_amounts = by_country \\\n",
    "        .mapPartitions(sum_sales) \\\n",
    "        .collect()\n",
    "    \n",
    "    print(\"Total sales for each partition: {}\".format(sum_amounts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using partitions in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "with SparkSession.builder \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 50) \\\n",
    "        .getOrCreate() as spark:\n",
    "    \n",
    "    rdd = spark.sparkContext \\\n",
    "        .parallelize(transactions) \\\n",
    "        .map(lambda x: Row(**x))\n",
    "    \n",
    "    df = spark.createDataFrame(rdd)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(df.rdd.glom().collect()))\n",
    "    \n",
    "    # Repartition by column\n",
    "    df2 = df.repartition(\"country\")\n",
    "    \n",
    "    print(\"\\nAfter 'repartition()'\")\n",
    "    print(\"Number of partitions: {}\".format(df2.rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(df2.rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(df2.rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SparkSession.builder \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .getOrCreate() as spark:\n",
    "    \n",
    "    nums_rdd = spark.sparkContext \\\n",
    "        .parallelize(nums) \\\n",
    "        .map(lambda x: Row(x))\n",
    "    \n",
    "    nums_df = spark.createDataFrame(nums_rdd, ['num'])\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(nums_df.rdd.getNumPartitions()))\n",
    "    print(\"Partitions structure: {}\".format(nums_df.rdd.glom().collect()))\n",
    "    \n",
    "    nums_df = nums_df.repartition(4)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(nums_df.rdd.getNumPartitions()))\n",
    "    print(\"Partitions structure: {}\".format(nums_df.rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SparkContext(\"local[2]\") as sc:\n",
    "    rdd = sc.parallelize(nums) \\\n",
    "        .map(lambda el: (el, el)) \\\n",
    "        .partitionBy(2) \\\n",
    "        .persist()\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "    \n",
    "    # Transform with `map()` \n",
    "    rdd2 = rdd.map(lambda el: (el[0], el[0]*2))\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd2.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd2.partitioner))  # We have lost a partitioner\n",
    "    print(\"Partitions structure: {}\".format(rdd2.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SparkContext(\"local[2]\") as sc:\n",
    "    rdd = sc.parallelize(nums) \\\n",
    "        .map(lambda el: (el, el)) \\\n",
    "        .partitionBy(2) \\\n",
    "        .persist()\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "    \n",
    "    # Use `mapValues()` instead of `map()` \n",
    "    rdd2 = rdd.mapValues(lambda x: x * 2)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd2.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd2.partitioner))  # We still got partitioner\n",
    "    print(\"Partitions structure: {}\".format(rdd2.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

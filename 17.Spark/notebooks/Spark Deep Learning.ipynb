{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloads flower photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://download.tensorflow.org/example_images/flower_photos.tgz\n",
    "#!tar xzf flower_photos.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dir = './flower_photos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates Spark session with appropriate packages to read JPEGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    ".Builder()\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training & test DataFrames for transfer learning - this piece of code is longer than transfer learning itself below!\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "tulips_df = ImageSchema.readImages(img_dir + \"/tulips\").withColumn(\"label\", lit(1))\n",
    "daisy_df = ImageSchema.readImages(img_dir + \"/daisy\").withColumn(\"label\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tulips_train, tulips_test = tulips_df.randomSplit([0.6, 0.4])\n",
    "daisy_train, daisy_test = daisy_df.randomSplit([0.6, 0.4])\n",
    "train_df = tulips_train.unionAll(daisy_train)\n",
    "test_df = tulips_test.unionAll(daisy_test)\n",
    "# Under the hood, each of the partitions is fully loaded in memory, which may be expensive.\n",
    "# This ensure that each of the paritions has a small size.\n",
    "train_df = train_df.repartition(100)\n",
    "test_df = test_df.repartition(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses InceptionV3 second-to-last layer as features to a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer \n",
    "\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
    "p = Pipeline(stages=[featurizer, lr])\n",
    "\n",
    "p_model = p.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "tested_df = p_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(tested_df.select(\"prediction\", \"label\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import expr, udf\n",
    "\n",
    "def _p1(v):\n",
    "    return float(v.array[1])\n",
    "p1 = udf(_p1, DoubleType())\n",
    "\n",
    "df = tested_df.withColumn(\"p_1\", p1(tested_df.probability))\n",
    "wrong_df = df.orderBy(expr(\"abs(p_1 - label)\"), ascending=False)\n",
    "wrong_df.select(\"image\", \"p_1\", \"label\").limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copies some photos to a sample folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_img_dir = './flower_photos/sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!mkdir ./flower_photos/sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!cp ./flower_photos/daisy/100080576_f52e8ee070_n.jpg ./flower_photos/sample\n",
    "#!cp ./flower_photos/daisy/10140303196_b88d3d6cec.jpg ./flower_photos/sample\n",
    "#!cp ./flower_photos/tulips/100930342_92e8746431_n.jpg ./flower_photos/sample\n",
    "#!cp ./flower_photos/tulips/10094729603_eeca3f2cb6.jpg ./flower_photos/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Makes predictions using InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import DeepImagePredictor\n",
    "\n",
    "image_df = ImageSchema.readImages(sample_img_dir)\n",
    "\n",
    "predictor = DeepImagePredictor(inputCol=\"image\", \n",
    "                               outputCol=\"predicted_labels\",\n",
    "                               modelName=\"InceptionV3\", \n",
    "                               decodePredictions=True, \n",
    "                               topK=10)\n",
    "predictions_df = predictor.transform(image_df)\n",
    "\n",
    "predictions_df.select(\"image\", \"predicted_labels\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p_model.transform(image_df)\n",
    "df.select(\"image\", (1-p1(df.probability)).alias(\"p_daisy\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users a Keras Pretrained model as a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "\n",
    "model = InceptionV3(weights=\"imagenet\")\n",
    "model.save('/tmp/model-full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StringType\n",
    "from sparkdl import KerasImageFileTransformer\n",
    "\n",
    "def loadAndPreprocessKerasInceptionV3(uri):\n",
    "    # this is a typical way to load and prep images in keras\n",
    "    image = img_to_array(load_img(uri, target_size=(299, 299)))  # image dimensions for InceptionV3\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return preprocess_input(image)\n",
    "\n",
    "transformer = KerasImageFileTransformer(inputCol=\"uri\", outputCol=\"predictions\",\n",
    "                                        modelFile='/tmp/model-full.h5',  # local file path for model\n",
    "                                        imageLoader=loadAndPreprocessKerasInceptionV3,\n",
    "                                        outputMode=\"vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "files = [os.path.join(sample_img_dir, f) for f in os.listdir(sample_img_dir)]\n",
    "uri_df = sqlContext.createDataFrame(files, StringType()).toDF(\"uri\")\n",
    "\n",
    "keras_pred_df = transformer.transform(uri_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = keras_pred_df.select(\"uri\", \"predictions\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(results.predictions.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses a regular Keras model as a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import KerasTransformer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Generate random input data\n",
    "num_features = 10\n",
    "num_examples = 100\n",
    "input_data = [{\"features\" : np.random.randn(num_features).astype(float).tolist()} for i in range(num_examples)]\n",
    "schema = StructType([ StructField(\"features\", ArrayType(FloatType()), True)])\n",
    "input_df = sqlContext.createDataFrame(input_data, schema)\n",
    "\n",
    "# Create and save a single-hidden-layer Keras model for binary classification\n",
    "# NOTE: In a typical workflow, we'd train the model before exporting it to disk,\n",
    "# but we skip that step here for brevity\n",
    "model = Sequential()\n",
    "model.add(Dense(units=20, input_shape=[num_features], activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model_path = \"/tmp/simple-binary-classification\"\n",
    "model.save(model_path)\n",
    "\n",
    "# Create transformer and apply it to our input data\n",
    "transformer = KerasTransformer(inputCol=\"features\", outputCol=\"predictions\", modelFile=model_path)\n",
    "final_df = transformer.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a Keras pretrained model as a UDF to be used on SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "from sparkdl.udf.keras_image_model import registerKerasImageUDF\n",
    "\n",
    "registerKerasImageUDF(\"inceptionV3_udf\", InceptionV3(weights=\"imagenet\"))\n",
    "registerKerasImageUDF(\"my_custom_keras_model_udf\", \"/tmp/model-full.h5\")\n",
    "\n",
    "def keras_load_img(fpath):\n",
    "    from keras.preprocessing.image import load_img, img_to_array\n",
    "    import numpy as np\n",
    "    img = load_img(fpath, target_size=(299, 299))\n",
    "    return img_to_array(img).astype(np.uint8)\n",
    "\n",
    "registerKerasImageUDF(\"inceptionV3_udf_with_preprocessing\", InceptionV3(weights=\"imagenet\"), keras_load_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_img_dir = './flower_photos/sample'\n",
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "image_df = ImageSchema.readImages(sample_img_dir)\n",
    "image_df.registerTempTable(\"sample_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT inceptionV3_udf(image) as predictions from sample_images\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT my_custom_keras_model_udf(image) as predictions from sample_images\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT inceptionV3_udf_with_preprocessing(image) as predictions from sample_images\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
